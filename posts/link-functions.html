<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Logistic Regression == Link(Linear Regression) ??</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="/css/water.min.css" />
  <!-- Cloudflare Web Analytics --><script defer src='https://static.cloudflareinsights.com/beacon.min.js' data-cf-beacon='{"token": "96199c1ca02344b698924c0e72918533"}'></script><!-- End Cloudflare Web Analytics -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
</head>
<body>
<header id="title-block-header">
<h1 class="title">Logistic Regression == Link(Linear Regression) ??</h1>
</header>
<h2 id="linear-regression">Linear Regression</h2>
<p>For Linear Regression, given a Dependent variable <code>y</code> that
is a Real number, we want to use the Independent variable
<code>x</code>, also a Real number, to predict <code>y</code>. The
formulation for Linear Regression is,</p>
<p><span class="math display">\[ y = w*x + b \]</span></p>
<p>Here, <code>w</code> and <code>b</code> are both real numbers as
well. On a 2 dimensional graph this equation will form a line and thus
the name “linear”. The linear regression model makes sense to many real
world problems because we know that there is “linearity” in the problem.
If the problem is not linear we can still try and fit a linear
regression model on the data, it just won’t work well when we try to use
the model in practice.</p>
<p>If we have a list of independent variables, we can rewrite this line
to a multi-dimensional object using a system of equations, using
matrices,</p>
<p><span class="math display">\[ Y = w^T*X + B \]</span></p>
<p>Here, <code>w</code> is the learnable parameter. We estimate the
value of <code>w</code> for a given dataset <code>D</code> of size
<code>n</code> typically with Ordinary Least Squares which is a closed
form solution and is the Maximum Likelihood estimate [1]. The key
assumptions here are that both <code>Y</code> and <code>X</code> are
continuous (Real numbers). Essentially Linear Regressions helps us model
a function that maps real numbers to real numbers (ie. <span
class="math inline">\(R \to R\)</span>). What if we want to model binary
or multi-label outputs?</p>
<h2 id="step-function">Step Function</h2>
<p>One way to do this is to use a step function. The extension is fairly
simple in that we take the Linear Regression model and apply a ‘sign’
[2] function to it, <span class="math display">\[ Y = sign\ \ (\ w^T*X +
B\ \ ) \]</span> <span class="math display">\[ sign(x) = \begin{cases}
1,\ x \ge 0 \\ 0,\ x &lt; 0 \end{cases} \]</span> The sign function here
captures the idea that if we output a positive value then we match that
to one of the <em>two</em> classes. For a 0 or negative value we assign
it to the opposite class.</p>
<p>The problem with this model is that the formulation might make sense
but it not work in practice. Linear regression works because it assume
linearity, but this is tied to the dependent variable. Some problems
happen to be linearly separable but most aren’t,</p>
<p>![[nonseparble-data.png]]</p>
<p>Non-linear regression can help but using the sign function limits
what we can do. We can however take the idea of using an intermediate
transformation on the regression and with the right choice of
transformation solve this problem. The idea of “link” functions are the
way to solve this, using logits.</p>
<h2 id="binary-logistic-regression">Binary Logistic Regression</h2>
<p>Let’s first look at learning a model that can predict a binary value
(ie 0 or 1). We know that we want a function that can map <span
class="math inline">\(R \to [0,1]\)</span>. The standard normal
distribution function is a good candidate and this idea is called the
Probit model [3], and around the same time the idea of using log odds
showed up [4].</p>
<p>For probability calculations <code>odds ratio</code> is an easy
measure to compute and communicate the chances of an event occurring
against it not occurring. For example,
<code>the odds of it raining today are 3 to 1</code>, which is to be
read as <code>it is 3 times more likely to rain than not</code>.</p>
<p>Why would you use this? Let’s say out of the past 4 days it rained 3
times versus 1. Here <code>p(rain) = 3/4</code> and
<code>p(~rain) = 1/4</code>. From a lay person’s perspective, it’s much
harder to communicate that the probability of rain is <code>0.75</code>
or <code>3/4</code>. It is simpler to just use
<code>3:1 or, 3 to 1</code>. The probability <code>0.75</code> is just
<code>3/(3+1)</code> calculated using the odds value, but much harder to
visualize. <code>3 to 1 odds in favour of raining</code> clearly
communicates that it rains 3 times more than it does not. Note, gambling
uses the term <code>odds</code> a little differently but it’s similar in
idea.</p>
<p>So given this context, the odds ratio helps use find the function we
care about. It is defined in terms of the probability of an event
as,</p>
<p><span class="math display">\[ odds = p / (1-p) \]</span></p>
<p>The odds ratio maps probability values to <span
class="math inline">\((0, +\infty)\)</span>. If we use the
<code>log</code> function on the odds we get a function that maps
probability values to <span class="math inline">\((-\infty,
+\infty)\)</span>. Log just happens to be the right function to extend
the probability mapping. Logits is a portmanteau of “log” and “digits”.
It just means “the log of numbers”, which comes from taking the logs of
the odds ratio.</p>
<p>To tie this all together, let’s call the linear regression on the
inputs <code>f(x)</code> and the dependent variable <code>y</code> which
represents the binary random variable.</p>
<p>For linear regression we are trying to learn params that will help us
model the dependent variable <code>y'</code> using the regression using
the following form,</p>
<p><span class="math display">\[ y&#39; = f(x) \ \ \ \ \ \ \ \ where \
f(x) = w*x + b\]</span></p>
<p>For a binary variable, <code>y</code>, we first apply the log odds
link function to y and then model the log odds using the regression. We
have a model of the binary dependent variable <code>y</code> in terms of
<code>x</code> just like we tried to with the <code>sign</code> function
above except we formulate it using an intermediate log odds step with a
rearrangement of terms,</p>
<p><span class="math display">\[
\begin{aligned}
log(\frac{y}{1-y}) = f(x) \\ \\
=&gt; \frac{y}{1-y} = e ^ {f(x)} \\
=&gt; y = \frac{e^{f(x)}}{(1 + e^{f(x)})} \\
=&gt; y = \frac{1}{(1 + e^{-f(x)})}
\end{aligned}
\]</span></p>
<p>Note, multi-label is not the same as multi-class. They sound similar
but are different. Multi-class means the dependent class can only be 1
of N classes. It cannot be two classes at the same time. This might seem
confusing because many real world objects are typically a combination of
multiple labels. To be clear, think of multi-class as predicting “an
orange or an apple” for a given image of a fruit. This means that it
<em>has</em> to be one or the other and the probability value is modeled
using log odds to reflect this. The reason we have <span
class="math inline">\(log \ (p / (1-p))\)</span> is to capture this
relationship. That is “it is either X or it is not X”.</p>
<p>Multi-label on the other hand specifically means the model outputs
are independent of each other and think of the classes as being “true”
at the same time. Instead of asking “is this an orange or an apple” in
the case of multi-class, we are asking “is this an apple?, is this an
orange?” in the multi-label setup. For multi-label regression, the
formulation is different and you can think of the setup as building a
regression classifier for each of the different labels (think of it as
<code>N</code> binary logistic regressions).</p>
<p>So, how do we extend Binary Logistic Regression to Multinomial (or
Multi-class Regression)?</p>
<h2 id="multi-class-logistic-regression">Multi-class Logistic
Regression</h2>
<p>For multi-class logistic regression we have to look back at what the
logit formulation is actually doing. When we start with <span
class="math inline">\(log(y/(1-y)) = f(x)\)</span> we are essentially
saying the “boundary line” between the two classes is <code>f(x)</code>.
In the discussion above <code>f(x)</code> is linear for discussion’s
sake but it can be a non-linear function as well which would help with
in cases like the circle dataset above. After rearranging the terms we
get a “logistic” separation boundary as we saw above.</p>
<p>So, if we think of the logits as representing the boundary between
two sets of data points, additional labels are simply pair-wise logits
as well with a caveat. The idea is that if we have <code>K</code>
classes, we pick one of them (say the last class for convenience), and
build logits between that class and every other class. This works
because we hold one class out and build multiple “logistic” separation
boundaries against it. Assuming <code>K=3</code> and using class
<code>3</code> as boundary, we have,</p>
<p><span class="math display">\[
\begin{aligned}
log \frac{P(y = i)}{P(y = K)} = w_i * x \\
=&gt; P_{y=1} = P_{y=3} * e^{w_1 * x}\ and \\
P_{y=2} = P_{y=3} * e^{w_2 * x}
\end{aligned}
\]</span></p>
<p>and by rearranging the terms,</p>
<p><span class="math display">\[
\begin{aligned}
P_{y=1} + P_{y=2} + P_{y=3} = 1 \\
=&gt; P_{y=3} = \frac{1}{1+e^{w_1 * x}+e^{w_2 * x}} \\
\end{aligned}
\]</span></p>
<p>Note here that class <code>3</code> doesn’t have a boundary with
itself, like the other classes do with the logits formulation. This
means that since it doesn’t exist we can set <span
class="math inline">\(w_3 = 0\)</span> and thus <span
class="math inline">\(e^{w_3*x} = 1\)</span>. So now we can rewrite
<span class="math inline">\(P_{y=2}\)</span> and the other probabilities
to,</p>
<p><span class="math display">\[
\begin{aligned}
P_{y=1} = \frac{e^{w_1 * x}}{e^{w_3 * x} + e^{w_1 * x}+e^{w_2 * x}} \\
\\
P_{y=2} = \frac{e^{w_2 * x}}{e^{w_3 * x}+e^{w_1 * x}+e^{w_2 * x}} \\
\\
P_{y=3} = \frac{e^{w_3 * x}}{e^{w_3 * x}+e^{w_1 * x}+e^{w_2 * x}} \\
\\
\\
\equiv P_{y=k} = \frac{e^{w_k * x}}{\Sigma^K_i e^{w_i * x}}
\end{aligned}
\]</span></p>
<p>Which is the <code>softmax</code> function as defined in ML
literature[5]</p>
<h2 id="references">References</h2>
<p>[1] <a
href="https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote08.html"
class="uri">https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote08.html</a></p>
<p>[2] <a href="https://en.wikipedia.org/wiki/Heaviside_step_function"
class="uri">https://en.wikipedia.org/wiki/Heaviside_step_function</a></p>
<p>[3] The origins and development of the logit model - J.S. Cramer,
University of Amsterdam and Tinbergen Institute, Amsterdam</p>
<p>[4] <a href="https://en.wikipedia.org/wiki/Probit_model"
class="uri">https://en.wikipedia.org/wiki/Probit_model</a></p>
<p>[5] Training Stochastic Model Recognition Algorithms as Networks can
lead to Maximum Mutual Information Estimation of Parameters - John S.
Bridle</p>
</body>
</html>
