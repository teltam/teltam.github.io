<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Softmax and Lagrange Multipliers</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="/css/water.min.css" />
  <!-- Cloudflare Web Analytics --><script defer src='https://static.cloudflareinsights.com/beacon.min.js' data-cf-beacon='{"token": "96199c1ca02344b698924c0e72918533"}'></script><!-- End Cloudflare Web Analytics -->
  <script
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
</head>
<body>
<header id="title-block-header">
<h1 class="title">Softmax and Lagrange Multipliers</h1>
</header>
<p>My understanding from a Machine Learning perspective is that Softmax
happens to be <em>a</em> function that can satisfy the requirements of a
multi-class probability distribution function,</p>
<ol type="1">
<li><code>f([x1, x2]) -&gt; [y1, y2, ...]</code> where y1 and y2 are
positive numbers.</li>
<li>Values in <code>[y1, y2, ...]</code> sum to 1.</li>
</ol>
<p>I didn’t realize you could analytically derive the Softmax function
using Entropy and Lagrange Multipliers. This helps me understand how a
Softmax at the end of a DNN converts the numerical output of a DNN into
a probability distribution.</p>
<h2 id="background">Background</h2>
<p>Given a probability distribution for a set of outcomes (classes), the
Shannon Entropy for this distribution quantifies how likely <em>a</em>
set of outcomes are if we were to observe them. It just so happens that
<code>log(1/p)</code> is the only function that satisfies the
constraints of the problem Shannon was trying to solve <a href="#fn1"
class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>,
and thus the formula for Shanon Entropy is,</p>
<p><span class="math display">\[ H(p) = - \Sigma_i \ p_i * log(p_i)
\]</span></p>
<p>Why do we care to maximize the entropy of the learned probability
distribution? <a href="#fn2" class="footnote-ref" id="fnref2"
role="doc-noteref"><sup>2</sup></a> Let’s say we have the following
discrete probability function <code>[0.25, 0.25, 0.25, 0.25]</code>.
That is, there are four classes and each of them are equally likely.
Here, <code>H = 1.3</code>, we have maximum entropy because we make no
assumptions about the likelihood of any class. A function
<code>[0.8, 0.1, 0.0009, 0.0001]</code>, with <code>H = 0.4</code>, has
some assumptions baked in because we expect the first class to show up
80% of the time.</p>
<p>This setup is called the Maximum Entropy Principle<a href="#fn3"
class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> and
we will use it in the derivation below.</p>
<h2 id="derivation">Derivation</h2>
<h3 id="main-optimization-problem">Main Optimization Problem</h3>
<p>Given <span class="math inline">\(z_i\)</span> as the raw output of a
DNN, and <span class="math inline">\(p_i = F(z_i)\)</span>, Find <span
class="math inline">\(F\)</span> that <em>maximizes</em> the given
function,</p>
<p><span class="math display">\[ H(p) = - \Sigma_i \ p_i * log(p_i)
\]</span> subject to, <span class="math display">\[ \Sigma_i \ p_i = 1
\]</span> <span class="math display">\[ \Sigma_i \ p_i * z_i = \mu
\]</span></p>
<p>The first constraint is to ensure the probability values sum to
1.</p>
<p>The second constraint is a bit harder to parse from an ML pov.
Derived from a Physics pov <a href="#fn4" class="footnote-ref"
id="fnref4" role="doc-noteref"><sup>4</sup></a>, this constraint is
simply a prior on what the probability distribution must look like. If
I’m working on this problem for the first time it’s not clear why I
should pick this constraint specifically, until you solve a slighly
simpler problem which we will do first below.</p>
<h3 id="simpler-optimization-problem">Simpler Optimization Problem</h3>
<p>Given <span class="math inline">\(z_i\)</span> as the raw output of a
DNN, and <span class="math inline">\(p_i = F(z_i)\)</span>, Find <span
class="math inline">\(F\)</span> that <em>maximizes</em> the given
function,</p>
<p><span class="math display">\[ H(p) = - \Sigma_i \ p_i * log(p_i)
\]</span> subject to, <span class="math display">\[ \Sigma_i \ p_i = 1
\]</span></p>
<p>The Lagrange optimization problem is,</p>
<p><span class="math display">\[ L = - \Sigma_i\  p_i log(p_i) -
\lambda_1 (\Sigma_i \ p_i -1) \]</span></p>
<p>Taking the derivative and setting to 0 you get, <span
class="math display">\[
∂L/∂p_i = -log(p_i) - 1 - λ₁ = 0
\]</span> and solving for $ p_i $ you get, <span class="math display">\[
\begin{matrix}
-log(p_i) = 1 + λ_1 \\
log(p_i) = -1 - λ_1 \\
=&gt; p_i = e^{(-1 - λ_1)} \\
\end{matrix}
\]</span> and applying the second constraint we get, <span
class="math display">\[
\begin{align}
\Sigma_i p_i = 1 \\
\Sigma_i e^{(-1 - λ_1)} = 1 \\
n * e^{(-1 - λ_1)} = 1 \\
=&gt; p_i = 1/n \\
\end{align}
\]</span></p>
<p>What this simpler optimization problem is telling us is that we need
some additional constraint on the output probability to force the
distribution values to be something other than the uniform probability
distribution.</p>
<p>The idea with the <span class="math inline">\(\mu\)</span> constraint
is that we want the probability distribution to have <em>some</em>
expected value. In the context of applied ML this just means we find the
best fit during training with the assumption that there is <em>some</em>
expected value and whatever the empirical value we can compute after
training is done is the expected value for that training run on the
dataset.</p>
<h3 id="solving-the-original-problem">Solving the Original Problem</h3>
<p>Given the constraints we have the following Lagrange optimization
problem, <span class="math display">\[
L = - \Sigma_i\  p_i log(p_i) - \lambda_1 (\Sigma_i \ p_i -1) -
\lambda_2(\Sigma_i p_i * z_i - \mu)
\]</span></p>
<p>Taking the derivative and setting to 0, we get, <span
class="math display">\[
\begin{align}
-log(p_i) = 1 + λ₁ + λ₂ * z_i \\
log(p_i) = -1 - λ₁ - λ₂ * z_i \\
p_i = exp(-1 - λ₁ - λ₂ * z_i) \\
=&gt; p_i = exp(-1 - λ₁) * exp(-λ₂ * z_i)
\end{align}
\]</span></p>
<p>Using the constraint 1 we get, $$ <span
class="math display">\[\begin{align}
\Sigma_i \ e^{(-1 - λ₁)} * e^{(-λ₂ * z_i)} = 1 \\
=&gt; e^{(-1 - λ₁)} = 1 / Σ e^{(-λ₂ * z_i)}

\end{align}\]</span> <span class="math display">\[
and finally substituting back,
\]</span> <span class="math display">\[\begin{align}
p_i = exp(-λ₂ * z_i) / Σ exp(-λ₂ * z_j)
\end{align}\]</span> $$</p>
<p>and if we set <span class="math inline">\(\lambda_2\)</span> to -1,
<span class="math display">\[
\begin{align}
=&gt; p_i = exp(z_i) / Σ exp(z_j)
\end{align}
\]</span></p>
<p>deriving the softmax formula.</p>
<h2 id="references">References</h2>
<section id="footnotes" class="footnotes footnotes-end-of-document"
role="doc-endnotes">
<hr />
<ol>
<li
id="fn1"><p>https://en.wikipedia.org/wiki/A_Mathematical_Theory_of_Communication<a
href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li
id="fn2"><p>https://en.wikipedia.org/wiki/Principle_of_maximum_entropy<a
href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li
id="fn3"><p>https://en.wikipedia.org/wiki/Principle_of_maximum_entropy<a
href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>https://en.wikipedia.org/wiki/Boltzmann_distribution<a
href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
</body>
</html>
